---
title             : "If mathematical psychology did not exist we would need to invent it: A case study in cumulative theoretical development in psychology"
shorttitle        : "Psychological theory"

author: 
  - name          : "Danielle J. Navarro"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "School of Psychology, University of New South Wales, Kensington 2052, Sydney, Australia"
    email         : "d.navarro@unsw.edu.au"

affiliation:
  - id            : "1"
    institution   : "School of Psychology, University of New South Wales"

authornote: |
  This manuscript is based on numerous conversations with Berna Dezever (and many others). I want to specifically note Berna's contribution in this initial submission as she will likely be a coauthor on any published version. At the current point in development she has not had the opportunity to provide input and -- as a way of assuming sole responsibilities for any errors in the current version -- I have not listed her as a coauthor at this stage. I would also like to apologise for the fact that the current version is not as polished as I would like for a submitted manuscript. Were it not for the special issue deadline the paper would not be submitted in this form, but writing was delayed considerably due to the COVID-19 outbreak.
  
abstract: |
  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sollicitudin nibh sit amet commodo. Malesuada pellentesque elit eget gravida cum. Magna fermentum iaculis eu non. Tempus imperdiet nulla malesuada pellentesque elit eget gravida. Donec massa sapien faucibus et molestie ac. Non blandit massa enim nec dui. Tellus in hac habitasse platea dictumst vestibulum rhoncus. Massa sed elementum tempus egestas sed sed. Pretium fusce id velit ut tortor pretium viverra suspendisse. At quis risus sed vulputate. Eget sit amet tellus cras adipiscing enim. Amet risus nullam eget felis. Lectus urna duis convallis convallis tellus id.

keywords          : "X"
wordcount         : "3300 in main text, 440 in references"

bibliography      : ["myrefs.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
ootnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf

header-includes   :  \usepackage{amsmath}
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\newpage
> *Those outside the field, on hearing the term mathematical psychology, often react with the raised eyebrows reserved for oxymora* [@shepard2004cognitive, p. 1]

\vspace*{12pt}


# Introduction

In 1987 Roger Shepard published a short paper in *Science* with the ambitious title "Toward a Universal Law of Generalization for Psychological Science" [@shepard1987toward]. Drawing on extensive work in the empirical literature on stimulus generalization across many species, he asserted the claim that the form of any stimulus generalization function shouold be approximately exponential in form, when measured with respect to an appropriately formulated stimulus representation. His paper begins with the following remark (p. 1317):

> The tercentenary of the publication, in 1687, of Newton's *Principia* prompts the question of whether psychological science has any hope of achieving a law that is comparable in generality (if not in predictive accuracy) to Newton's universal law of gravitation. Exploring the direction that currently seems most favorable for an affirmative answer, I outline empirical evidence an a theoretical rationale in support of a tentative candidate for a universal law of generalization

\noindent
Shepard's claim in the original paper was remarkable in scope. He drew on data from several terrestrial species (e.g., humans, pigeons, rats) and across many stimulus domains (e.g., visual, auditory), data that had hitherto been considered unrelated. To spot the connection between these data, Shepard used statistical insights from the similarity modelling literature. He noted that the apparent noninvariance of observed stimulus generalisation functions stemmed largely from the fact that response data were previously analysed with respect to the physical dissimilarities of the stimulus. When the same responses were replotted as a function of distance in a psychological space contructed by multidimensional scaling, he found that the form of the stimulus generalisation was remarkably regular in shape, as shown on the left side of Figure 1.

(ref:shepard-caption) Two of the figures from Shepard's (1987) paper on stimulus generalisation. On the left, his Fig 1 shows the empirical evidence for an exponential law and on the right his Fig 3 outlines his theoretical claim. [Note: higher resolution versions will be provided in the final version and permission will be obtained to reproduce the originals]
```{r, echo=FALSE, fig.cap = "(ref:shepard-caption)", fig.pos = "t"}
knitr::include_graphics("shepard.jpg",dpi = 150)
```

Taken by itself Shepard's empirical discovery would have been impressive. However, not content merely to reanalyse existing the data to discover the underlying invariance and thereby unify several distinct strands of research, Shepard went on to provide a theoretical explanation for *why* we should expect to find this invariance. The theory was surprisingly simple: the learner presumes there exists some unknown *consequential* region of the stimulus space across which roughly the same properties hold (e.g., things that look like apples will probably taste the same as one another). Encountering a single stimulus that entails a particular consequence, the learner's task is to infer the location, shape and size of the consequential region itself. Naturally this is an underconstrained problem, as there are an infinite number of possible regions that might correspond to the true consequential region. Nevertheless, Shepard showed that under a quite range of assumptions that the learner might make about the nature of consequential regions, the shape of the *generalization* function across the stimulus space ends up approximately exponential, shown on the right side of Figure 1.

Since its publication in 1987, Shepard's paper has been cited approximately 2500 times according to Google Scholar, no small feat for a theoretical paper that presented no new empirical data and whose content is almost entirely devoted to the derivation of a formal relation between one unobservable quantity (distance between two items psychological space) and another (the probability two items fall in the same region). The universal law featured prominently in a special issue of Brain and Behavior Sciences in 2001, as well as a first person retrospective written 17 years later [@shepard2004cognitive]. It is rightly considered one of the most important works in mathematical psychology, and indeed all of coognitive science. In this special issue devoted to the prospects for theoretical development in psychology, I can think of no better case study to motivate our remarks. My goal in this paper is to discuss the role that theory can play in the development of psychological science, using Shepard's law to illustrate.

# What a theory is not

Reading Shepard's original 1987 paper and the 2004 retrospective, some surprising characteristics of his theoretical work stand out. First, the theoretical development was largely post hoc. The paper does not collect new data, and indeed the main empirical results reported in the paper (left hand side of Figure 1) were based on a reanalysis of existing data. Second, the paper reports no hypothesis tests or indeed any statistical inference of any kind. There are no p-values, no Bayes factors, nor any confidence intervals or their Bayesian equivalents. Third, the paper does not outline any specific novel prediction about the result of future experiments. Though the paper makes a strong claim that the exponential law should hold broadly it does prescribe how tests of this prediction should be constructed. 

Given the current climate in psychological science, all three of these might seem somewhat surprising, perhaps even a form of ``questionable'' research practice. In the current zeitgeist, statistical tests are considered essential, post hoc theorising is a questionable research practice, and prediction is considered the central goal of theory. Judged by the standards currently used to assess psychological research Shepard's paper fares quite poorly. One might be tempted to ask how Shepard managed to get the paper published at all, much less to have it established as one of the most influential theoretical works in cognitive science! Or alternatively, to take a different perspective, perhaps these standards are ill-suited to the job of assessing theoretical work. 

Indeed, one might argue that the success of Shepard's law tells us a great deal about what a theory is not. Novel data, no matter how fascinating or carefully collected, do not make a theoretical contribution. A statistical test, no matter how well constructed, does not encapsulate what a theory does. Perhaps surprisingly, a theory need not make novel predictions. Shepard's theoretical contribution was not the discovery of the exponential law, but rather to explain it. What Shepard's paper did was *systematise* an existing body of empirical findings, separating those aspects to the data that are invariant across studies from that which is not, and provided a precisely stated mathematical formalism that did more than merely summarise the data. Shepard's theory asserts that the form of generalization curves are exponential, but the exponential form is not a theoretical primitive. The theoretical primitives are the assumptions Shepard made about consequential regions, psychological spaces, and the relationship these things have to stimulus generalization. The exponential law is an *entailment* of the theory, not its substance. 

Empirical data, statistical inference and novel predictions are all vital to the scientific process -- of that there is little doubt -- but none of them are substitutes for or synonymous with theoretical development. If so, what might a theory be? 

# Distinguishing theories from measurement tools

It is not difficult to list some of the desiderata for a scientific theory. A scientific theory should be *independent of its creator*, for instance. It is difficult to make much use of a theory otherwise. In practice this typically means a theory is mathematical or computational in nature. Similarly, psychological theories should of course make some connection with empirical data, giving some account of the *generative mechanism* that gave rise to those data. Theories should be usable, in the sense of providing other scientists *guidance for future research*. Other criteria could also be named, including falsifiability, simplicity, compatibility with existing literature, generalizability, predictive ability and so on. However, while it is easy to list desiderata and even easier to argue over which elements to such lists are the most important, such ``discussions in the abstract'' rarely provide much guidance to the would-be theoretician. From the perspective of the working scientist, it is perhaps more useful to give concrete examples, and to that end I return to an examination of Shepard's (1987) paper.

One of the key insights in Shepard (1987) is the recognition that stimulus generalization functions that appear extremely irregular at first glance are in fact highly regular when stimuli are represented in an appropriate fashion, as depicted in Figure 2. Generalization to tones is irregular when tones are described in terms of their ordering around the chroma circle, but very smooth when described around the circle of fifths. Similarly, stimulus generalization to colours is irregular with respect the physical wavelenghts of light but is smooth with respect to perceptual colours space [e.g., @ekman1954dimensions]. In retrospect this seems an obvious point to cognitive scientists who have built from Shepard's work, and moreover in keeping with philosophical work on the representation of similarity [@goodman1972seven]. However, at the time Shepard developed the theory, he was faced with a substantive problem of how to extract the *appropriate* stimulus representation to apply a theory to. Nonmetric multidimensional scaling [@kruskal1964nonmetric] as a measurement instrument tool served this purpose for Shepard in 1987, and the smooth empirical curves on the left of Figure 1 all use MDS-estimated psychological spaces to supply the relevant measure of distance on the x-axis. 

(ref:shepard2-caption) Figure 5 from Shepard's (2004) retrospective account of the universal law, plotting data from previous experiments in the original form (left) and after those same data were recast in terms of constructs relevant to the theory (right). [Note: a higher resolution version will be provided in the final version and permission will be obtained to reproduce the originals]
```{r, echo=FALSE, fig.cap = "(ref:shepard2-caption)", fig.pos = "t"}
knitr::include_graphics("shepard2.jpg",dpi = 300)
```
  
Clearly, without MDS as a measurement tool, Shepard would have found it almost impossible to formulate the empirical regularity with any confidence. However, it is equally clear that MDS is merely a tool used to help define the phenomenon to be explained. It does not define the theory itself, and it quickly became apparent that the scope of Shepard's law applies in situations where MDS does not: shortly after the publication of Shepard's original paper, @russell1988analogy demonstrated that the same law holds for stimuli defined in terms of discrete features as well as to the continuous spaces for which Shepard's work was defined, a connection that was later extended by @tenenbaum2001generalization. While the theoretical framework could not have come into existence without the scaffolding provided by the MDS measurement model, it quickly outgrew any need for this support. Illustrating this, the generalization gradient shown in Figure 3 is neither exponential in superficial form, nor is the underlying stimulus representation a metric space extracted by MDS, yet as @tenenbaum2001generalization note, it too can be seen as an entailment of Shepard's theoretical framework. 

(ref:tenenbaum1-caption) Figure 5 from Tenenbaum and Griffiths' (2001) paper *Generalization, similarity, and Bayesian inference*. [Note: a higher resolution version will be provided in the final version and permission will be obtained to reproduce the originals]
```{r, echo=FALSE, fig.cap = "(ref:tenenbaum1-caption)", fig.pos = "t"}
knitr::include_graphics("tenenbaum_figure5.png",dpi = 300)
```



# Extensibility of a good theory

The original theory of generalization presented by Shepard (1987) was not explicitly framed as Bayesian model of cognition. In his original notation (Equation 2 from the paper) Shepard proposed that the generalization strength for a stimulus located at coordinate $\mathbf{x} = (x_1, \ldots, x_K)$ in a $K$-dimensional space,^[The space is usually endowed with one of the Minkowski distance metrics, most typically the Euclidean distance measure or the city block distance. One of the major contributions Shepard made was to provide a theoretical explanation for why the distance metric varies across stimulus domains, but is a little beyond the scope of the current paper.] when the learner has previously encountered a consequential stimulus located at the origin be expressed as follows:

\begin{equation}
g(\mathbf{x}) = \int_0^\infty p(s) \frac{m(s, \mathbf{x})}{m(s)} ds
\end{equation}

\noindent
where the ratio $m(s, \mathbf{x})/m(s)$ is a measure describing the probability that a consequential region of size $s$ contains $\mathbf{x}$ given that it also contains the consequential stimulus at the origin, and $p(s)$ is a probability measure that assigns value to regions of size $s$. Much of Shepard's paper is devoted to solving this integral for various choices of $p(s)$ and showing that the resulting generalization function $g(\mathbf{x})$ is approximately exponential in form, and is precisely exponential when $p(s)$ is the Erlang distribution.

The connection to Bayesian inference was made explicit by @tenenbaum2001generalization who recast Shepard's formalism in the following fashion. Where Shepard referred to the notion of a *consequential region* located within a psychological space (with all the geometric connotations that entails), Tenenbaum and Griffiths relaxed this to be a more general *consequential set*. Any specific candidate for the true consequential set was labelled a *hypothesis* $h$, presumed to belong to a broader *hypothesis space* $\mathcal{H}$. In their notation, the measure on regions sizes $p(s)$ was replaced by a Bayesian prior $p(h)$ defined over the hypothesis space, and Shepard's ratio $m(s, \mathbf{x})/m(s)$ recast in probabilistic terms also. In their notation, the probability of generalising to a novel stimulus $y$ given that a consequential stimulus $x$ has already been observed is formulated as the following Bayesian marginal probability:

\begin{equation}
p(y \in C | x) = \sum_{h : y \in C} p(h|x)
\end{equation}
\noindent
where $p(h|x)$ is the posterior probability a Bayesian learner assigns to hypothesis $h$ in light of observing the consequential stimulus $x$,
\begin{equation}
p(h|x) = \frac{p(x|h)p(h)}{\sum_{h^\prime \in \mathcal{H}} p(x | h^\prime) p(h^\prime)}
\end{equation}

The Bayesian reformulation of Shepard's theory presented by Tenenbaum and Griffiths was more than a superficial change: it allowed them to show how Shepard's theory could itself be generalized in three distinct ways. First, as mentioned earlier, they showed (much like Russell 1988) that Shepard's theory could encompass stimuli that were not representable as points in a metric space: in their notation, this is as accomplished by substituting a new hypothesis space $\mathcal{H}$. Second, this formulation allowed the theory to naturally accommodate inductive generalization problems in which the learner has encountered more than one consequential stimulus. If a set of $n$ stimuli $\mathbf{x} = (x_1, \ldots, x_n)$ are generated in a (conditionally) independent fashion, the likelihood $p(\mathbf{x}|h)$ factorises and thus
\begin{equation}
p(\mathbf{x}|h) = \prod_{i=1}^n p(x_i |h).
\end{equation}
Finally, this formalism called attention to a potentially limiting assumption in Shepard's original paper. Shepard (1986, p. 1321) argued that ``in the absence of any information to the contrary, an individual might best assume that nature selects the consequential region and the first stimulus independently''. According to this account, which Tenenbaum and Griffiths termed *weak sampling*, this independence gives a uniform likelihood function:
\begin{equation}
p(x|h) \propto 1
\end{equation}
They contrasted this with a *strong sampling* account in which stimuli are presumed to be sampled uniformly from the consequential set. If $|h|$ denotes the size of the consequential set^[For finite sets this $|h|$ is simply the number of entities in the set; for infinite sets, a more careful approach to measuring sets is required] then a strong sampling process produces the likelihood
\begin{equation}
p(x|h) = 1/|h|
\end{equation}
if $x$ is an element of the consequential set, and 0 otherwise. 

# The blessing of abstraction

In retrospect, one of the most important contributions made by the general Bayesian formulation adopted by Tenenbaum and Griffiths (2001) is that it allowed the underlying theory to be applied in a much broader range of scenarios. Shepard's (1987) original construction, though purportedly to be a very general law itself, was formulated with respect to a narrow class of psychological problems: inductive generalization from a single observation. Moreover, because the origins of his work lay in the study of human perception and the animal learning literature, it was not immediately clear (at least not to me) how the theory should be extended to higher order cognition. By framing the problem in a more general way, Tenenbaum and Griffiths' construction enabled a large proportion of my later research. Examples of research questions that I would have been unable to properly formulate without this theory:

- What kind of individual differences exist in the sampling assumptions people adopt in simple generalization problems? [@navarro2012sampling]
- Do categorization problems and inductive generalization problems lead people to adopt different sampling assumptions? [@hendrickson2019sample]
- Do people's beliefs about the origins of data and shape how we interpret the evidentiary value of the quantity [@ransom2016leaping] and diversity of evidence [@hayes2019diversity]?
- Can we use the Bayesian theory to unify and extend previously puzzling results [@lawson2009sample] on how sampling affects reasoning? [@hayes2019selective] 
- Can we build a Bayesian model of how people ``take a hint'' from ostensibly uninformative data? [@voorspoels2015people] 
- Completing a ``virtuous cycle'' of sorts, recent work has taken our results from the reasoning context [@voorspoels2015people] and applied it to make novel predictions in the associative learning [@lee2019negative]

This is of course a highly selected portion of the relevant literature, focusing as it does on work in which I was personally involved. The purpose of choosing these examples is that -- as the person developing the theoretical extensions in each case -- I am in a position to comment on exactly *how* the theoretical formalism from Tenenbaum and Griffiths (2001) paper enabled this research when the original formalism from Shepard (1987) would have made it difficult. Because Shepard's formalism adhered closely to the specific problem he sought to explain (generalization from a single observed stimulus represented in a metric space) it was not clear to me prior to 2001 that the learner is necessarily reliant on an *assumption* (explicit or implicit) about how the stimulus was called to the learner's attention. Under the Bayesian reformulation of the theory, this assumption was rendered visible via the likelihood $P(x|h)$. When written in this form, it was painfully clear to me that the inductive inferences any learner makes about a novel item $y$ is necessarily dependent on more than the observed data $x$ -- it also depends on the assumptions embodied in the likelihood $P(\cdot|h)$ as to how those data were collected. This link, present but obscured in Shepard's (1987) paper, was the central motivation behind this entire body of work and would not have been possible without the more abstract formalism introduced by Tenenbaum and Griffiths (2001). 

# Conclusion

Although I have focused on Shepard's law and its extensions in this paper, the underlying pattern is -- I would suggest -- quite general. I could have chosen the Rescorla-Wagner model of associative learning as the basis for this discussion [@rescorla1972theory], or the Generalized Context Model of human categoriztion [@nosofsky1986attention]. I could have chosen to focus on models such as ALCOVE that sought to unify associative learning and categorization [@kruschke1992alcove], or models such as the hierarchical Dirichlet process that seek to unify a wide variety of category learning models within a common theoretical language [@griffiths2007unifying]. I could have revisited Ebbinghaus' 1885 work on formal theory in experimental psychology [reprinted as @ebbinghaus2013memory]. I could have examined sequential sampling models of choice reaction time [@luce1986response] and the rich theoretical tradition that mathematical psychologists have developed in that domain also. 

These theoretical advances have something in common. In each of these areas psychological researchers have built up a considerable body of theoretical knowledge that is instantiated in formal models of psychological processes. In every case the underlying theoretical models are more than mere summaries of empirical results, and more substantive than a mere statistical model. In all cases the formalism can be used to generate novel predictions in experimental paradigms that differ markedly from the experimental contexts used to develop the model (and, remarkably, some of those predictions have even turned out to be correct). By a judicious combination of abstraction and formalism, mathematical psychologists have been able to develop a toolkit that allows anyone to derive theoretical predictions in completely novel paradigms. If it is indeed the cases that psychology suffers from a kind of ``theoretical amnesia'' [@boorsbaum2013theoretical], perhaps the apparatus of mathematical psychology will be of value elsewhere in the discipline. Perhaps fittingly, the words of Shepard (1987) seem an appropriate way to conclude

> Undoubtably, psychological science has lagged by behind physical science by at least 300 years. Undoubtedly, too, prediction of behavior can never attain the precision for animate that it has for celestial bodies. Yet, psychology may not be inherently limited merely to the descriptive characterization of the behaviors of particular terrestrial species. Possibly, behind the diverse behaviors of humans and animals, as behind the various motions of planets and stars, we may discern the operation of universal laws

# References

```{r create_r-references}
r_refs(file = "myrefs.bib")
```

<!-- \begingroup -->
<!-- \setlength{\parindent}{-0.5in} -->
<!-- \setlength{\leftskip}{0.5in} -->

<!-- <div id = "refs"></div> -->
<!-- \endgroup -->
